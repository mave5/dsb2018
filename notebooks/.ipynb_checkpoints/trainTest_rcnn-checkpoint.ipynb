{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras: 2.1.3\n",
      "numpy: 1.13.3\n",
      "matplotlib: 2.1.0\n",
      "cv2: 2.4.13\n",
      "theano: 0.9.0.dev-425cb8effc7958e8ca376b023d8344b7620a9622\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu0,floatX=float32\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "import time\n",
    "import datetime\n",
    "from image import ImageDataGenerator\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 42\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "\n",
    "# get package versions\n",
    "def get_version(*vars):\n",
    "    for var in vars:\n",
    "        module = __import__(var)    \n",
    "        print ('%s: %s' %(var,module.__version__))\n",
    "    \n",
    "# package version    \n",
    "get_version('keras','numpy','matplotlib','cv2','theano')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "TRAIN_PATH = '../data/stage1_train/'\n",
    "TEST_PATH = '../data/stage1_test/'\n",
    "\n",
    "# normalization type\n",
    "#norm_type='zeroMeanUnitStd'\n",
    "norm_type=None\n",
    "\n",
    "netinfo='trainTest_rcnn'\n",
    "initialLearningRate=3e-4\n",
    "\n",
    "\n",
    "# Get train and test IDs\n",
    "train_ids = next(os.walk(TRAIN_PATH))[1]\n",
    "test_ids = next(os.walk(TEST_PATH))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data generator\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.15,\n",
    "        height_shift_range=0.15,\n",
    "        shear_range=0.05,\n",
    "        zoom_range=0.05,\n",
    "        channel_shift_range=0.0,\n",
    "        fill_mode='constant',\n",
    "        cval=0.0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,)\n",
    "        #data_format=\"channels_first\") \n",
    "    \n",
    "# calcualte dice\n",
    "def calc_dice(X,Y,d=0):\n",
    "    N=X.shape[d]    \n",
    "    # intialize dice vector\n",
    "    dice=np.zeros([N,1])\n",
    "\n",
    "    for k in range(N):\n",
    "        x=X[k,0] >.5 # convert to logical\n",
    "        y =Y[k,0]>.5 # convert to logical\n",
    "\n",
    "        # number of ones for intersection and union\n",
    "        intersectXY=np.sum((x&y==1))\n",
    "        unionXY=np.sum(x)+np.sum(y)\n",
    "\n",
    "        if unionXY!=0:\n",
    "            dice[k]=2* intersectXY/(unionXY*1.0)\n",
    "            #print 'dice is: %0.2f' %dice[k]\n",
    "        else:\n",
    "            dice[k]=1\n",
    "            #print 'dice is: %0.2f' % dice[k]\n",
    "        #print 'processing %d, dice= %0.2f' %(k,dice[k])\n",
    "    return np.mean(dice),dice\n",
    "\n",
    "def preprocess(X,xnormType=None):\n",
    "    if xnormType=='minus1plus1':\n",
    "        X=X.astype('float32')\n",
    "        X/=np.max(X)\n",
    "        X-=0.5\n",
    "        X=X*2\n",
    "    elif xnormType=='zeroMeanUnitStd':\n",
    "        X=X.astype('float32')\n",
    "        # we do this per channel\n",
    "        for c in range(X.shape[1]):\n",
    "            X[:,c]-=np.mean(X[:,c])\n",
    "            stdXc=np.std(X[:,c])\n",
    "            if stdXc>0.0:\n",
    "                X[:,c]/=stdXc\n",
    "    elif xnormType is None:\n",
    "        pass\n",
    "    else:\n",
    "        raise IOError('normalization type not found!')\n",
    "    return X\n",
    "\n",
    "def gammaAug(images,gamma):\n",
    "    # images shape: N*C*H*W\n",
    "    #print ('before gamma:', np.mean(image))\n",
    "    n1,c1,_,_=images.shape\n",
    "    for k1 in range(n1):\n",
    "        #for k2 in range(c1):\n",
    "        g = (2 * np.random.rand() - 1) * gamma + 1.\n",
    "        images[k1]=(images[k1]) ** g\n",
    "    #print ('after gamma:', np.mean(image))\n",
    "    return images\n",
    "\n",
    "\n",
    "def array_stats(X):\n",
    "    X=np.asarray(X)\n",
    "    print ('array shape: ',X.shape, X.dtype)\n",
    "    #print 'min: %.3f, max:%.3f, avg: %.3f, std:%.3f' %(np.min(X),np.max(X),np.mean(X),np.std(X))\n",
    "    print ('min: {}, max: {}, avg: {:.3}, std:{:.3}'.format( np.min(X),np.max(X),np.mean(X),np.std(X)))\n",
    "\n",
    "def resizeY(Y,origHW):\n",
    "    # Y shape is N*1*H*W\n",
    "    N=Y.shape[0] \n",
    "    Yr=[]\n",
    "    for k in range(N):\n",
    "        temp=resize(Y[k,0], (origHW[k,0], origHW[k,1]), mode='constant', preserve_range=True)\n",
    "        \n",
    "        #Yr.append(np.array(temp,'uint8'))\n",
    "        Yr.append(temp)\n",
    "    return Yr\n",
    "\n",
    "def resizeX(X,origHW):\n",
    "    # Y shape is N*1*H*W\n",
    "    N=X.shape[0] \n",
    "    Xr=[]\n",
    "    for k in range(N):\n",
    "        Xk=X[k]\n",
    "        Xk=np.transpose(Xk,(1,2,0))\n",
    "        temp=resize(Xk, (origHW[k,0], origHW[k,1]), mode='constant', preserve_range=True)\n",
    "        temp=np.transpose(temp,(2,0,1))\n",
    "        Xr.append(np.array(temp,'uint8'))\n",
    "    return Xr\n",
    "\n",
    "def loadData0(id_,path2data):\n",
    "    #print('loading '+path2data)\n",
    "    path = path2data + id_   \n",
    "    #print(path)\n",
    "    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
    "    #h,w,c=img.shape\n",
    "    origHW=img.shape\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "\n",
    "    # collecting masks\n",
    "    mask = []\n",
    "    try:\n",
    "        for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
    "            mask_ = imread(path + '/masks/' + mask_file)\n",
    "            mask_ = resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "            mask.append(mask_)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return img,np.array(mask),origHW\n",
    "\n",
    "\n",
    "def loadData(path2h5,data_type='train'):\n",
    "    if not os.path.exists(path2h5):\n",
    "        h5file=h5py.File(path2h5,'w-')\n",
    "        for id_ in train_ids:\n",
    "            img,mask,origHW=loadData0(id_,TRAIN_PATH)            \n",
    "            #print img.shape,mask.shape,origHW\n",
    "            grp=h5file.create_group(id_)\n",
    "            grp['X']=img\n",
    "            grp['Y']=mask\n",
    "            grp['origHW']=origHW\n",
    "            \n",
    "        \n",
    "        for id_ in test_ids:\n",
    "            img,mask,origHW=loadData0(id_,TEST_PATH)            \n",
    "            grp=h5file.create_group(id_)\n",
    "            grp['X']=img\n",
    "            grp['Y']=mask\n",
    "            grp['origHW']=origHW\n",
    "        \n",
    "    else:\n",
    "        print('loading '+ path2h5)\n",
    "        h5file=h5py.File(path2h5,'r')\n",
    "    return h5file\n",
    "\n",
    "def iterate_minibatches(inputs1 , targets,  batchsize, shuffle=True, augment=True):\n",
    "    assert len(inputs1) == len(targets)\n",
    "    if augment==True:\n",
    "        if shuffle:\n",
    "            indices = np.arange(len(inputs1))\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, len(inputs1) - batchsize + 1, batchsize):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batchsize]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            x = inputs1[excerpt]\n",
    "            y = targets[excerpt] \n",
    "            for  xxt,yyt in datagen.flow(x, y , batch_size=x.shape[0]):\n",
    "                x = xxt.astype(np.float32) \n",
    "                y = yyt \n",
    "                break\n",
    "    else:\n",
    "        x=inputs1\n",
    "        y=targets\n",
    "\n",
    "    #yield np.array(x,np.uint8), np.array(y, dtype=np.uint8)         \n",
    "    return np.array(x,np.uint8), np.array(y, dtype=np.uint8)     \n",
    "    \n",
    "    \n",
    "# train test model\n",
    "def train_test_model(X_train,y_train,X_test,y_test,params_train):\n",
    "    foldnm=params_train['foldnm']  \n",
    "    pre_train=params_train['pre_train'] \n",
    "    batch_size=params_train['batch_size'] \n",
    "    augmentation=params_train['augmentation'] \n",
    "    path2weights=params_train['path2weights'] \n",
    "    path2model=params_train['path2model'] \n",
    "    norm_type=params_train['norm_type'] \n",
    "    gamma=params_train['gamma'] \n",
    "    \n",
    "    print('batch_size: %s, Augmentation: %s' %(batch_size,augmentation))\n",
    "    \n",
    "    print 'fold %s training in progress ...' %foldnm\n",
    "    # load last weights\n",
    "    if pre_train== True:\n",
    "        if  os.path.exists(path2weights):\n",
    "            model.load_weights(path2weights)\n",
    "            print 'previous weights loaded!'\n",
    "        else:\n",
    "            raise IOError('weights does not exist!!!')\n",
    "    else:\n",
    "        if  os.path.exists(path2weights):\n",
    "            model.load_weights(path2weights)\n",
    "            print (path2weights)\n",
    "            print ('previous weights loaded!')\n",
    "            train_status='previous weights'\n",
    "            return train_status\n",
    "    \n",
    "    # path to csv file to save scores\n",
    "    path2scorescsv = weightfolder+'/scores.csv'\n",
    "    first_row = 'train,test'\n",
    "    with open(path2scorescsv, 'w+') as f:\n",
    "        f.write(first_row + '\\n')\n",
    "           \n",
    "    # Fit the model\n",
    "    start_time=time.time()\n",
    "    scores_test=[]\n",
    "    scores_train=[]\n",
    "    if params_train['loss']=='dice': \n",
    "        best_score = 0\n",
    "        previous_score = 0\n",
    "    else:\n",
    "        best_score = 1e6\n",
    "        previous_score = 1e6\n",
    "    patience = 0\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    \n",
    "    \n",
    "    for epoch in range(params_train['nbepoch']):\n",
    "    \n",
    "        print ('epoch: %s,  Current Learning Rate: %.1e' %(epoch,model.optimizer.lr.get_value()))\n",
    "        #seed = np.random.randint(0, 999999)\n",
    "    \n",
    "        if augmentation:\n",
    "            X_batch,Y_batch=iterate_minibatches(X_train,y_train,X_train.shape[0],shuffle=False,augment=True)  \n",
    "            X_batch=gammaAug(X_batch,gamma) # gamma/brightness augmentation\n",
    "            Y_batch=Y_batch[:,0][:,np.newaxis]\n",
    "            #model.fit_generator(train_generator, steps_per_epoch=len(xtr)/batch_size, epochs=1,verbose=0)            \n",
    "            hist=model.fit(preprocess(X_batch,norm_type), Y_batch, batch_size=batch_size,epochs=1, verbose=0)\n",
    "        else:\n",
    "            hist=model.fit(preprocess(X_train,norm_type), y_train, batch_size=batch_size,epochs=1, verbose=0)\n",
    "            \n",
    "        # evaluate on test and train data\n",
    "        score_test=model.evaluate(preprocess(X_test,norm_type),y_test,verbose=0)\n",
    "        score_train=np.mean(hist.history['loss'])\n",
    "       \n",
    "        print ('score_train: %s, score_test: %s' %(score_train,score_test))\n",
    "        scores_test=np.append(scores_test,score_test)\n",
    "        scores_train=np.append(scores_train,score_train)    \n",
    "\n",
    "        # check for improvement    \n",
    "        if (score_test<=best_score):\n",
    "            print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!! viva, improvement!!!!!!!!!!!!!!!!!!!!!!!!!!!\") \n",
    "            best_score = score_test\n",
    "            patience = 0\n",
    "            model.save_weights(path2weights)  \n",
    "            model.save(path2model)\n",
    "            \n",
    "        # learning rate schedule\n",
    "        if score_test>previous_score:\n",
    "            #print \"Incrementing Patience.\"\n",
    "            patience += 1\n",
    "\n",
    "        # learning rate schedule                \n",
    "        if patience == params_train['max_patience']:\n",
    "            params_train['learning_rate'] = params_train['learning_rate']/2\n",
    "            print (\"Upating Current Learning Rate to: \", params_train['learning_rate'])\n",
    "            model.optimizer.lr.set_value(params_train['learning_rate'])\n",
    "            print (\"Loading the best weights again. best_score: \",best_score)\n",
    "            model.load_weights(path2weights)\n",
    "            patience = 0\n",
    "        \n",
    "        # save current test score\n",
    "        previous_score = score_test    \n",
    "        \n",
    "        # store scores into csv file\n",
    "        with open(path2scorescsv, 'a') as f:\n",
    "            string = str([score_train,score_test])\n",
    "            f.write(string + '\\n')\n",
    "           \n",
    "    \n",
    "    print ('model was trained!')\n",
    "    elapsed_time=(time.time()-start_time)/60\n",
    "    print ('elapsed time: %d  mins' %elapsed_time)      \n",
    "\n",
    "    # train test progress plots\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(scores_test)\n",
    "    plt.plot(scores_train)\n",
    "    plt.title('train-validation progress',fontsize=20)\n",
    "    plt.legend(('test','train'),fontsize=20)\n",
    "    plt.xlabel('epochs',fontsize=20)\n",
    "    plt.ylabel('loss',fontsize=20)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(weightfolder+'/train_val_progress.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print 'training completed!'\n",
    "    train_status='completed!'\n",
    "    return train_status    \n",
    "\n",
    "def grays_to_RGB(img):\n",
    "    # turn 2D grayscale image into grayscale RGB\n",
    "    return np.dstack((img, img, img))\n",
    "\n",
    "\n",
    "def image_with_mask(img, mask,color=(0,255,0)):\n",
    "    mask=np.asarray(mask,dtype='uint8') \n",
    "    \n",
    "    if len(img.shape)==2:\n",
    "        img_color = grays_to_RGB(img)\n",
    "    else:\n",
    "        img_color =img\n",
    "\n",
    "    mask2=mask[:,:,0]\n",
    "    for c1 in range(mask.shape[2]):\n",
    "        mask2=np.logical_or(mask2,mask[:,:,c1])\n",
    "    mask2=np.array(255*mask2,'uint8')\n",
    "        \n",
    "    mask_edges = cv2.Canny(mask2, 100, 200) > 0\n",
    "    #plt.imshow(mask_edges)\n",
    "    maximg=np.max(img)\n",
    "    img_color[mask_edges, 0] = maximg*color[0]  # set channel 0 to bright red, green & blue channels to 0\n",
    "    img_color[mask_edges, 1] = maximg*color[1]\n",
    "    img_color[mask_edges, 2] = maximg*color[2]\n",
    "    return img_color\n",
    "\n",
    "def disp_img_2masks(img,mask1=None,mask2=None,r=1,c=1,d=0,indices=None):\n",
    "    if mask1 is None:\n",
    "        mask1=np.zeros(img.shape,dtype='uint8')\n",
    "    else:\n",
    "        mask1=np.array(mask1,dtype='uint8')\n",
    "    if mask2 is None:\n",
    "        mask2=np.zeros(img.shape,dtype='uint8')\n",
    "    else:        \n",
    "        mask2=np.array(mask2,dtype='uint8')    \n",
    "        \n",
    "    N=r*c    \n",
    "    if d==2:\n",
    "        # convert to N*C*H*W\n",
    "        img=np.transpose(img,(2,0,1))\n",
    "        img=np.expand_dims(img,axis=1)\n",
    "        \n",
    "        mask1=np.transpose(mask1,(2,0,1))\n",
    "        mask1=np.expand_dims(mask1,axis=1)\n",
    "\n",
    "        mask2=np.transpose(mask2,(2,0,1))\n",
    "        mask2=np.expand_dims(mask2,axis=1)\n",
    "        \n",
    "    if indices is None:    \n",
    "        indices=np.random.randint(img.shape[0],size=N)\n",
    "    \n",
    "    # collect images and masks\n",
    "    I1=[np.transpose(img[i],(1,2,0)) for i in indices]\n",
    "    M1=[np.transpose(mask1[i],(1,2,0)) for i in indices]\n",
    "    M2=[np.transpose(mask2[i],(1,2,0)) for i in indices]\n",
    "    \n",
    "    C1=(0,255,0)\n",
    "    C2=(255,0,0)\n",
    "    for k in range(N):    \n",
    "        imgmask=image_with_mask(I1[k],M1[k],C1)\n",
    "        imgmask=image_with_mask(imgmask,M2[k],C2)\n",
    "        plt.subplot(r,c,k+1)\n",
    "        plt.imshow(imgmask)\n",
    "        plt.title(indices[k])\n",
    "    plt.show()            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../data/trainTestH256W256withIds.hdf5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAE/CAYAAADvxHzxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2dcf985810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path2h5='../data/trainTestH'+str(IMG_HEIGHT)+'W'+str(IMG_WIDTH)+'withIds.hdf5'\n",
    "h5file=loadData(path2h5,'train')\n",
    "\n",
    "# number of masks in each id\n",
    "numOfMasksPerId=[]\n",
    "for id_ in train_ids:\n",
    "    numOfMasksPerId.append(h5file[id_]['Y'].shape[0])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(numOfMasksPerId,100)\n",
    "plt.title('number of masks per id');\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "disp_img_2masks(img=X,mask1=Y,r=2,c=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, merge, Convolution2D, Deconvolution2D, AtrousConvolution2D, MaxPooling2D, UpSampling2D, Flatten, Dropout, Dense\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import layer_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import AtrousConv2D, MaxPooling2D, UpSampling2D, Flatten, Dropout, Dense\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Activation,Reshape,Permute\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import layer_utils\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU,PReLU,LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def conv2dcustom(x_input,filters=8,kernel_size=3,strides=1,w2reg=None,pool=False,padding='same',batchNorm=False,activation='relu'):\n",
    "    data_format='channels_first'\n",
    "    x1 = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding,data_format=data_format,kernel_regularizer=w2reg,strides=strides)(x_input)\n",
    "    if batchNorm==True:\n",
    "        x1=BatchNormalization(axis=1)(x1)\n",
    "        \n",
    "    if activation=='leaky':        \n",
    "        x1 = LeakyReLU(0.1)(x1)\n",
    "    else:\n",
    "        conv1=Activation('relu')(conv1)        \n",
    "\n",
    "    if pool==True:\n",
    "        x1=MaxPooling2D(pool_size=(2, 2),data_format=data_format)(x1)\n",
    "    \n",
    "    return x1\n",
    "\n",
    "\n",
    "# model\n",
    "def model_skip(params):\n",
    "\n",
    "    h=params['h']\n",
    "    w=params['w']\n",
    "    z=params['z']\n",
    "    lr=params['learning_rate']\n",
    "    loss=params['loss']\n",
    "    C=params['initial_channels']\n",
    "    nb_output=params['num_labels']\n",
    "    dropout_rate=params['dropout_rate']\n",
    "    data_format='channels_first'\n",
    "    batchNorm=params['batchNorm']\n",
    "    w2reg=params['w2reg']\n",
    "    initStride=params['initStride']\n",
    "    reshape4softmax=params['reshape4softmax']\n",
    "    \n",
    "    \n",
    "    inputs = Input((z,h, w))\n",
    "    conv1=conv2dcustom(filters=C,x_input=inputs,strides=initStride,w2reg=w2reg,activation='leaky')    \n",
    "    pool1=conv2dcustom(filters=C,x_input=conv1,w2reg=w2reg,pool=True,activation='leaky')    \n",
    "\n",
    "    conv2=conv2dcustom(filters=2*C,x_input=pool1,w2reg=w2reg,activation='leaky')    \n",
    "    pool2=conv2dcustom(filters=2*C,x_input=conv2,w2reg=w2reg,pool=True,activation='leaky')    \n",
    "    \n",
    "    conv3=conv2dcustom(filters=4*C,x_input=pool2,w2reg=w2reg,activation='leaky')    \n",
    "    pool3=conv2dcustom(filters=4*C,x_input=conv3,w2reg=w2reg,pool=True,activation='leaky')    \n",
    "\n",
    "    conv4=conv2dcustom(filters=8*C,x_input=pool3,w2reg=w2reg,activation='leaky')    \n",
    "    pool4=conv2dcustom(filters=8*C,x_input=conv4,w2reg=w2reg,pool=True,activation='leaky')    \n",
    "\n",
    "    conv5=conv2dcustom(filters=16*C,x_input=pool4,w2reg=w2reg,activation='leaky')    \n",
    "    conv5=conv2dcustom(filters=16*C,x_input=conv5,w2reg=w2reg,pool=False,activation='leaky')    \n",
    "    \n",
    "    # dropout\n",
    "    conv5 =Dropout(dropout_rate)(conv5)\n",
    "    \n",
    "    up7=UpSampling2D(size=(2, 2),data_format=data_format)(conv5)\n",
    "    concat = Concatenate(axis=1)\n",
    "    up7 = concat([up7, conv4])\n",
    "\n",
    "    conv7=conv2dcustom(filters=8*C,x_input=up7,w2reg=w2reg,pool=False,activation='leaky')    \n",
    "    \n",
    "    up8 = concat([UpSampling2D(size=(2, 2),data_format=data_format)(conv7), conv3])\n",
    "\n",
    "    conv8=conv2dcustom(filters=4*C,x_input=up8,w2reg=w2reg,pool=False,activation='leaky')        \n",
    "    \n",
    "    up9 = concat([UpSampling2D(size=(2, 2),data_format=data_format)(conv8), conv2])\n",
    "    \n",
    "    conv9=conv2dcustom(filters=2*C,x_input=up9,w2reg=w2reg,pool=False,activation='leaky')        \n",
    "\n",
    "    up10 = concat([UpSampling2D(size=(2, 2),data_format=data_format)(conv9), conv1])\n",
    "    conv10=conv2dcustom(filters=C,x_input=up10,w2reg=w2reg,pool=False,activation='leaky')        \n",
    "\n",
    "    conv10 = UpSampling2D(size=(initStride, initStride),data_format=data_format)(conv10)\n",
    "    conv10=conv2dcustom(filters=C,x_input=conv10,w2reg=w2reg,pool=False,activation='leaky')        \n",
    "    \n",
    "    conv10 = Conv2D(nb_output, 1, data_format=data_format,kernel_regularizer=w2reg)(conv10)\n",
    "\n",
    "    if reshape4softmax:\n",
    "        # reshape for softmax\n",
    "        output=Reshape((nb_output,h*w)) (conv10)\n",
    "        # permute for softmax\n",
    "        output=Permute((2,1))(output)\n",
    "        # softmax\n",
    "        output=Activation('softmax')(output)\n",
    "    else:        \n",
    "        output=Activation('sigmoid')(conv10)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    if loss=='dice':\n",
    "        model.compile(optimizer=Adam(lr), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    else:\n",
    "        #model.compile(loss='binary_crossentropy', optimizer=Adam(lr))\n",
    "        model.compile(loss=loss, optimizer=Adam(lr))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,z,h,w=X.shape\n",
    "params_train={\n",
    "    'h': h,\n",
    "    'w': w,\n",
    "    'z':z,\n",
    "    'learning_rate': initialLearningRate,\n",
    "    'optimizer': 'Adam',\n",
    "    #'loss': 'categorical_crossentropy',\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'nbepoch': 400 ,\n",
    "    'num_labels': 1,\n",
    "    'initial_channels':16,\n",
    "    'dropout_rate': 0.5,\n",
    "    'max_patience': 30,\n",
    "    'experiment': None,\n",
    "    'pre_train': False,\n",
    "    'elastic_arg': None,\n",
    "    'trainaug_params': None,\n",
    "    'batch_size': 10,\n",
    "    'weightfolder': None,\n",
    "    'w2reg': l2(1e-4),    \n",
    "    'batchNorm': False,\n",
    "    'initStride': 2,\n",
    "    'norm_type': norm_type,\n",
    "    'augmentation': True,\n",
    "    'reshape4softmax': False,\n",
    "    'gamma': 0.3,\n",
    "    }\n",
    "    \n",
    "model = model_skip(params_train)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=5\n",
    "skf = ShuffleSplit(n_splits=n_folds, test_size=0.1, random_state=321)\n",
    "\n",
    "# loop over folds\n",
    "foldnm=0\n",
    "scores_nfolds=[]\n",
    "\n",
    "print ('wait ...')\n",
    "for train_ind, test_ind in skf.split(X,Y):\n",
    "    foldnm+=1    \n",
    "\n",
    "    train_ind=list(np.sort(train_ind))\n",
    "    test_ind=list(np.sort(test_ind))\n",
    "    \n",
    "    X_train,Y_train=X[train_ind],np.array(Y[train_ind],'uint8')\n",
    "    X_test,Y_test=X[test_ind],np.array(Y[test_ind],'uint8')\n",
    "    \n",
    "    array_stats(X_train)\n",
    "    array_stats(Y_train)\n",
    "    array_stats(X_test)\n",
    "    array_stats(Y_test)\n",
    "    print ('-'*30)\n",
    "\n",
    "    # exeriment name to record weights and scores\n",
    "    experiment=netinfo+'_hw_'+str(h)+'by'+str(w)+'_initfilts_'+str(params_train['initial_channels'])\n",
    "    print ('experiment:', experiment)\n",
    "\n",
    "    weightfolder='./output/weights/'+experiment+'/fold'+str(foldnm)\n",
    "    if  not os.path.exists(weightfolder):\n",
    "        os.makedirs(weightfolder)\n",
    "        print ('weights folder created')    \n",
    "    \n",
    "    # path to weights\n",
    "    path2weights=weightfolder+\"/weights.hdf5\"\n",
    "    path2model=weightfolder+\"/model.hdf5\"    \n",
    "    \n",
    "    # train test on fold #\n",
    "    params_train['foldnm']=foldnm\n",
    "    params_train['learning_rate']=initialLearningRate\n",
    "    params_train['path2weights']=path2weights\n",
    "    params_train['path2model']=path2model\n",
    "    model=model_skip(params_train)\n",
    "    #model.summary()    \n",
    "    train_test_model(X_train,Y_train,X_test,Y_test,params_train)\n",
    "    \n",
    "    # loading best weights from training session\n",
    "    if  os.path.exists(path2weights):\n",
    "        model.load_weights(path2weights)\n",
    "        print 'weights loaded!'\n",
    "    else:\n",
    "        raise IOError('weights does not exist!!!')\n",
    "    \n",
    "    score_test=model.evaluate(preprocess(X_test,norm_type),Y_test,verbose=0,batch_size=8)\n",
    "    print ('score_test: %.5f' %(score_test))    \n",
    "    Y_pred=model.predict(preprocess(X_test,norm_type))>0.5\n",
    "    dicePerFold,_=calc_dice(Y_test,Y_pred)\n",
    "    print('average dice: %.2f' %dicePerFold)\n",
    "    print ('-' *30)\n",
    "    # store scores for all folds\n",
    "    scores_nfolds.append(score_test)\n",
    "\n",
    "print ('average score for %s folds is %s' %(n_folds,np.mean(scores_nfolds)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diaply predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "disp_img_2masks(X_test,Y_test,Y_pred,r=2,c=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display after resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_testResized=resizeY(Y_pred,HW_train[test_ind])\n",
    "X_testResized=resizeX(X_test,HW_train[test_ind])\n",
    "array_stats(Y_testResized[0])\n",
    "array_stats(X_testResized[0])\n",
    "\n",
    "#disp_img_2masks(X_test,Y_test,Y_pred,r=2,c=3)\n",
    "n1=np.random.randint(len(X_testResized))\n",
    "disp_img_2masks(X_testResized[n1][np.newaxis],Y_testResized[n1][np.newaxis,np.newaxis],r=1,c=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_testR=resizeY(Y_test,HW_train[test_ind])\n",
    "y1=Y_testR[1]\n",
    "y2=Y_testResized[1]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(y1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(y2)\n",
    "plt.show()\n",
    "\n",
    "lab_img1 = label(y1 > .5)\n",
    "lab_img2 = label(y2 > .5)\n",
    "print (np.max(lab_img1))\n",
    "print (np.max(lab_img2))\n",
    "\n",
    "for i in range(1, lab_img.max() + 1):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(lab_img1==i)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(lab_img2==i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Leaderboard Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_leader,_,HW_leader=loadData(path2h5,'leader')\n",
    "array_stats(X_leader)\n",
    "array_stats(HW_leader)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(HW_leader[:,0])\n",
    "plt.title('H')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(HW_leader[:,1])\n",
    "plt.title('W')\n",
    "plt.show()\n",
    "\n",
    "n_folds=5\n",
    "# prediction for nfolds\n",
    "Y_leaderAllFolds=[]\n",
    "for foldnm in range(1,n_folds+1):\n",
    "    print('fold: %s' %foldnm)\n",
    "    # load weights\n",
    "    experiment=netinfo+'_hw_'+str(h)+'by'+str(w)+'_initfilts_'+str(params_train['initial_channels'])\n",
    "    print ('experiment: %s' %experiment)\n",
    "    weightfolder='./output/weights/'+experiment+'/fold'+str(foldnm)\n",
    "    # path to weights\n",
    "    path2weights=weightfolder+\"/weights.hdf5\"\n",
    "    if  os.path.exists(path2weights):\n",
    "        model.load_weights(path2weights)\n",
    "        print ('%s loaded!' %path2weights)\n",
    "    else:\n",
    "        raise IOError ('weights does not exist!')\n",
    "\n",
    "    # prediction\n",
    "    Y_leader_perfold=model.predict(preprocess(X_leader.value,norm_type))\n",
    "    array_stats(Y_leader_perfold)\n",
    "    Y_leaderAllFolds.append(Y_leader_perfold)        \n",
    "    print('-'*50)\n",
    "\n",
    "# convert to array\n",
    "Y_leader=np.hstack(Y_leaderAllFolds)\n",
    "print ('ensemble shape:', Y_leader.shape)\n",
    "Y_leader=np.mean(Y_leader,axis=1)[:,np.newaxis]\n",
    "array_stats(Y_leader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "disp_img_2masks(X_leader,Y_leader>0.5,r=2,c=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display after resizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_leaderResized=resizeY(Y_leader>0.5,HW_leader)\n",
    "X_leaderResized=resizeX(X_leader,HW_leader)\n",
    "array_stats(Y_leaderResized[0])\n",
    "array_stats(X_leaderResized[0])\n",
    "\n",
    "#disp_img_2masks(X_test,Y_test,Y_pred,r=2,c=3)\n",
    "n1=np.random.randint(len(X_leaderResized))\n",
    "array_stats(Y_leaderResized[n1])\n",
    "array_stats(X_leaderResized[n1])\n",
    "print n1,test_ids[n1]\n",
    "disp_img_2masks(X_leaderResized[n1][np.newaxis],Y_leaderResized[n1][np.newaxis,np.newaxis],r=1,c=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run-length encoding stolen from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "def prob_to_rles(x, cutoff=0.5):\n",
    "    lab_img = label(x > cutoff)\n",
    "    for i in range(1, lab_img.max() + 1):\n",
    "        yield rle_encoding(lab_img == i)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_stats(Y_leader)\n",
    "Y_leaderResized=resizeY(Y_leader,HW_leader)\n",
    "array_stats(Y_leaderResized[0])\n",
    "\n",
    "new_test_ids = []\n",
    "rles = []\n",
    "for n, id_ in enumerate(test_ids):\n",
    "    rle = list(prob_to_rles(Y_leaderResized[n]))\n",
    "    rles.extend(rle)\n",
    "    new_test_ids.extend([id_] * len(rle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame\n",
    "sub = pd.DataFrame()\n",
    "sub['ImageId'] = new_test_ids\n",
    "sub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "info=experiment\n",
    "suffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "submissionFolder='./output/submissions'\n",
    "if not os.path.exists(submissionFolder):\n",
    "    os.mkdir(submissionFolder)\n",
    "    print(submissionFolder+ ' created!')\n",
    "path2submission = os.path.join(submissionFolder, 'submission_' + suffix + '.csv')\n",
    "print(path2submission)\n",
    "sub.to_csv(path2submission, index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
